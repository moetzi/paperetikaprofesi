# -*- coding: utf-8 -*-
"""SVM ETI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9n2SqwL7aRTlR5KHkyR7KDC6RWCZfK9
"""

import pandas as pd
import re
import nltk

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

"""Dataset"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

dataset = 'drive/MyDrive/ETI/Translated.csv'
data = pd.read_csv(dataset)
data

data.dropna(inplace=True)

data

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('vader_lexicon')
nltk.download('wordnet')

data

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from nltk.tokenize import word_tokenize

lemma = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def CleanText(txt):
    # Remove hashtag words (words starting with #)
    txt = re.sub(r'#\w+', ' ', txt)
    # Remove non-alphabetical characters and convert to lowercase
    txt = re.sub('[^a-zA-Z]', ' ', txt).lower()
    # Tokenize text
    txt = word_tokenize(txt)
    # Remove stopwords and lemmatize each word (keeping only verbs)
    txt = [lemma.lemmatize(word, pos='v') for word in txt if word not in stop_words and len(word) > 3]
    # Join cleaned tokens into a single string
    txt = ' '.join(txt)
    return txt

stop_words

data['clean_tweets'] = data['full_text'].apply(CleanText)

# Display a sample of the cleaned reviews to verify
print("Original tweets:")
print(data.loc[0, 'full_text'])  # Display original review
print("\nCleaned Review:")
print(data.loc[0, 'clean_tweets'])  # Display cleaned review

data

# # Install nltk if not already installed
# !pip install nltk

# Import necessary libraries
import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize the SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

# Assuming 'data' is already defined as your DataFrame with 'clean_tweets' column
# Apply sentiment analysis directly on the 'clean_tweets' column
data['sentiment_scores'] = data['clean_tweets'].apply(lambda x: analyzer.polarity_scores(x))
data['compound_score'] = data['sentiment_scores'].apply(lambda x: x['compound'])

# Determine sentiment label based on compound score
data['predicted_label'] = data['compound_score'].apply(lambda score: 'positive' if score > 0 else 'negative' if score < 0 else 'neutral')

# Display the updated DataFrame with sentiment analysis results
data

new_data = data[['clean_tweets', 'label']]
new_data

new_data.to_csv('/content/cleantext.csv', index=False)

x = data['full_text']
y = data['label']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10)

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.svm import LinearSVC
# from sklearn.metrics import accuracy_score

# x = data['clean_tweets']
# y = data['label']

# # Define a range of test sizes to evaluate
# test_sizes = [0.1, 0.15, 0.2, 0.25, 0.3, 0.4]

# # Initialize variables to track best test size and highest accuracy
# best_test_size = None
# highest_accuracy = 0.0

# # Iterate over each test size
# for test_size in test_sizes:
#     # Split the data into training and testing sets
#     x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42)

#     # Initialize TF-IDF vectorizer
#     tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed

#     # Fit TF-IDF vectorizer on training data and transform training data
#     x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)

#     # Transform test data using the same vectorizer
#     x_test_tfidf = tfidf_vectorizer.transform(x_test)

#     # Initialize and train a LinearSVC model
#     model = LinearSVC(random_state=42)
#     model.fit(x_train_tfidf, y_train)

#     # Make predictions on the test set
#     y_pred = model.predict(x_test_tfidf)

#     # Calculate accuracy
#     accuracy = accuracy_score(y_test, y_pred)

#     # Print accuracy for the current test size
#     print(f"Test Size: {test_size}, Accuracy: {accuracy}")

#     # Check if current accuracy is the highest encountered so far
#     if accuracy > highest_accuracy:
#         highest_accuracy = accuracy
#         best_test_size = test_size

# # Print the best test size and corresponding highest accuracy
# print(f"\nBest Test Size: {best_test_size}, Highest Accuracy: {highest_accuracy}")

x_train

from sklearn.feature_extraction.text import TfidfVectorizer

# Create TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_df = 0.8, min_df = 2)

# Fit and transform the training data (x_train)
x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)

# Transform the test data (x_test) using the same vectorizer
x_test_tfidf = tfidf_vectorizer.transform(x_test)

# vectorizer = CountVectorizer()
# vectorizer.fit(x_train)

# x_train = vectorizer.transform(x_train)
# x_test = vectorizer.transform(x_test)

# x_train.toarray()

from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

for c in [0.01, 0.05, 0.25, 0.5, 0.75, 1]:
  svm = LinearSVC(C=c)
  svm.fit(x_train_tfidf, y_train)
  print('Akurasi untuk c = %s: %s' %(c, accuracy_score(y_test, svm.predict(x_test_tfidf))))

svm = LinearSVC(C = 0.75)
svm.fit(x_train_tfidf, y_train)

print('Accuracy score model final: %s ' %accuracy_score(y_test, svm.predict(x_test_tfidf)))

# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import classification_report

# y_pred = svm.predict(x_test_tfidf)
# print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm.score(x_test_tfidf, y_test)))

# confusion_matrix = confusion_matrix(y_test, y_pred)
# print(confusion_matrix)
# print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

y_pred = svm.predict(x_test_tfidf)
print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm.score(x_test_tfidf, y_test)))

# Calculate accuracy of SVM classifier
accuracy = svm.score(x_test_tfidf, y_test)
print('Accuracy of SVM classifier on test set: {:.2f}'.format(accuracy))

# Calculate confusion matrix and classification report
cm = confusion_matrix(y_test, y_pred)
# print(cm)
print(classification_report(y_test, y_pred))

# Confusion Matrix
class_label = ["negative", "neutral", "positive"]
df_cm = pd.DataFrame(cm, index=class_label, columns=class_label)
sns.heatmap(df_cm, annot=True, fmt='d')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()