# -*- coding: utf-8 -*-
"""Paper ETI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lu7_ZT80uGgA5LWpQ9h6yObeFX-k_AXj

## **1. Data Scraping**
"""

# Import required Python package
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

from google.colab import drive
drive.mount('/content/drive')

datakw = 'drive/MyDrive/ETI/keyword.csv'
data = pd.read_csv(datakw)
data.head()

#Memasukkan auth_token dari X/Twitter

twitter_auth_token = 'iniadalahauth_tokentwiteranda'

# Mencari data dari twitter dengan keyword tertentu
# Keyword dibaca per baris dalam file csv
import pandas as pd

# Batasi jumlah hasil yang diambil
limit = 500

# Baca file CSV dengan pandas
data1 = pd.read_csv(datakw)

# Iterate through each row of the DataFrame
for index, row in data1.iterrows():
    keyword = row["keyword"]
    filename = f'{keyword}.csv'
    # Set the username, keyword, language, and date range
    search_keyword = f'{keyword} lang:id since:2023-03-01 until:2024-05-08'

    !npx --yes tweet-harvest@2.2.7 -o "{filename}" -s "{search_keyword}" -l {limit} --token {twitter_auth_token}

    print(f"Searching tweets for keyword: {keyword}")

import os
import pandas as pd

# Path ke folder "tweets-data"
folder_path = '/content/tweets-data'

# Inisialisasi DataFrame kosong untuk menyimpan hasil penggabungan
merged_df = pd.DataFrame()

# Iterate through each file in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.csv'):  # Hanya proses file CSV
        file_path = os.path.join(folder_path, filename)
        # Baca file CSV dan tambahkan ke DataFrame
        df = pd.read_csv(file_path, delimiter=";")
        # # Tambahkan kolom baru yang berisi asal file CSV
        # df['suburb'] = filename
        merged_df = pd.concat([merged_df, df], ignore_index=True)

# Simpan DataFrame hasil penggabungan menjadi satu file CSV
merged_file_path = '/content/merged_tweets.csv'
merged_df.to_csv(merged_file_path, index=False)

print("All CSV files have been merged into one with source column:", merged_file_path)

# Specify the path to your CSV file
file_path = '/content/merged_tweets.csv'

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path)

# Display the DataFrame
df.head()

# Cek jumlah data yang didapatkan

num_tweets = len(df)
print(f"Jumlah tweet dalam dataframe adalah {num_tweets}.")

"""# **2. Data Preprocessing**

Data Preprocessing adalah kegiatan pengolahan data agar dapat dioperasikan secara lebih lanjut. Secara umum, kegiatan ini bisa disebut juga dengan "data cleansing" dimana kita mengolah data yang kotor (value yang hilang, duplikat, outlier) menjadi data bersih.

Data kotor termasuk:
- Data duplikat: satu baris data yang muncul lebih dari satu kali
- Data tidak lengkap: beberapa field berisi null value
- Data tidak konsisten: data dalam format yang salah atau format yang berbeda-beda
- Data tidak akurat: Data yang di-input tidak benar, seperti misspelled

## Dataset 1
"""

# Read csv files to Dataframe
df.head()

# Check information of dataframE
df.info()

# menampilkan mean, standar deviasi, min, kuartil (25%, 50%, 75%) dan max (pandas)
df.describe()
# df.describe(include='all') statistical description of ALL COLUMNS, both NUMERIC/STRING datatype

"""`duplicated()` akan menunjukkan baris mana yang terduplikasi (nilai tiap kolomnya sama dengan baris sebelum-sebelumnya)

Ketika `sum()` ditambahkan, akan diberikan output berupa jumlah (sum) dari baris-baris data yang mengandung null value.
"""

# Mencari data duplikat
print("Jumlah duplikasi: ", df.duplicated().sum())

"""Kita dapat mengatasi duplikasi data dengan menghapus data yang terduplikat.

`drop_duplicates()` akan menghapus baris-baris yang terduplikasi.
"""

# Membersihkan data duplikasi
df.drop_duplicates(inplace=True)
print("Jumlah duplikasi: ", df.duplicated().sum())

# Memeriksa missing value
df.isna().sum()

"""`isna()` akan memberikan output berupa baris-baris data mana saja yang mengandung missing value di dalamnya.

Missing values termasuk:
* None
* NaN (Not a Number)
* Empty strings

Cara mengatasi missing value:
- Menghapus missing value/sparse columns (kolom yang berisi banyak null)
- Mengisi missing value dengan nilai mayoritas (mean, median, mode)
"""

df['username'].value_counts()

"""`value_counts()` akan memberikan output berupa frekuensi suatu nilai muncul.

Data yang diambil hanya berasal dari username @dfes_wa sehingga apabila ada data dengan usernaame lain, harus kita drop
"""

# Menyimpan / menyalin dataframe berisi kolom yang diperlukan
df_filtered = df[['full_text', 'lang']]
df_filtered

# import re

# def clean_text(text):
#     text = re.sub(r'http\S+', ' ', text)
#     text = re.sub(r'#[a-zA-Z0-9_]+', ' ', text)
#     text = re.sub(r'@[a-zA-Z0-9_]+', ' ', text)
#     text = re.sub('[^a-zA-Z]', ' ', text).lower()
#     return text
# df_filtered['cleaned_text'] = df_filtered['full_text'].apply(lambda x: clean_text(x))
# df_filtered

import re

def clean_text(text):
    text = re.sub(r'RT\S+', ' ', text)
    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'(.)\1+', r'\1\1', text)
    text = re.sub(r'#[a-zA-Z0-9_]+', ' ', text)
    text = re.sub(r'@[a-zA-Z0-9_]+', ' ', text)
    text = re.sub('[^a-zA-Z]', ' ', text).lower()
    return text
df_filtered['cleaned_text'] = df_filtered['full_text'].apply(lambda x: clean_text(x))
df_filtered

filtered_df = df_filtered[
    (df_filtered['cleaned_text'].str.len() > 90) &  # Filter by text length
    (~df_filtered['cleaned_text'].str.contains('wts', case=False)) &  # Exclude rows containing 'wts'
    (~df_filtered['cleaned_text'].str.contains('ip', case=False)) &  # Exclude rows containing 'ip'
    (~df_filtered['cleaned_text'].str.contains('wto', case=False)) &  # Exclude rows containing 'wto'
    (~df_filtered['cleaned_text'].str.contains('iphone', case=False)) &  # Exclude rows containing 'iphone'
    (~df_filtered['cleaned_text'].str.contains('jastip', case=False)) &  # Exclude rows containing 'jastip'
    (~df_filtered['cleaned_text'].str.contains('ready', case=False)) & # Exclude rows containing 'ready'
    (~df_filtered['cleaned_text'].str.contains('order', case=False)) & # Exclude rows containing 'order'
    (~df_filtered['cleaned_text'].str.contains('joki', case=False)) & # Exclude rows containing 'joki'
    (~df_filtered['cleaned_text'].str.contains('slot', case=False)) & # Exclude rows containing 'slot'
    (~df_filtered['cleaned_text'].str.contains('alpha', case=False)) & # Exclude rows containing 'alpha' - it's binance related things
    (~df_filtered['cleaned_text'].str.contains('po', case=False))  # Exclude rows containing 'po'
]

filtered_df

# Mencari data duplikat
print("Jumlah duplikasi: ", filtered_df['cleaned_text'].duplicated().sum())

data1 = filtered_df['cleaned_text']
data1

# Membersihkan data duplikasi
data1.drop_duplicates(inplace=True)
print("Jumlah duplikasi: ", data1.duplicated().sum())

data1

# !pip install Sastrawi

# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# # Initialize StopWordRemoverFactory
# stop_factory = StopWordRemoverFactory()

# # Define additional stop words as a set
# more_stopwords = {'dengan', 'ia', 'bahwa', 'oleh', 'becuk', 'dancuk', 'ajg', }

# # Get default stop words as a list from StopWordRemoverFactory
# default_stopwords = stop_factory.get_stop_words()

# # Convert the set of custom stop words to a list
# custom_stopwords_list = list(more_stopwords)

# # Combine default stop words list with custom stop words list
# combined_stopwords = default_stopwords + custom_stopwords_list

# # Print the combined list of stop words
# print(combined_stopwords)

# # Create StopWordRemover
# stopword = stop_factory.create_stop_word_remover()

# # Example usage: Remove stop words from a sample text
# sample_text = "Ini adalah contoh kalimat dengan beberapa kata yang tidak penting seperti bahwa dan oleh"
# clean_text = stopword.remove(sample_text)
# print(clean_text)

filtered_df.to_csv('/content/tinggaltranslate.csv', index=False)